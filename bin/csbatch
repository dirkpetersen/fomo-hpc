#! /usr/bin/python3

"""
# csbatch - reads Slurm submission scripts and executes them in AWS EC2
"""

import sys
import os
import time
import argparse
import re
import subprocess
import warnings
import pwd
import grp
import getpass
import boto3
import botocore.exceptions
from botocore.exceptions import ClientError
from collections import defaultdict
from pathlib import Path
from typing import Dict, Any, List, Tuple

# List of implemented SBATCH options that can be set via CLI
active_sbatch_options = [
    'job_name', 'comment', 'cpus_per_task', 'chdir', 
    'mail_type', 'mail_user', 'ntasks', 'output',
    'error', 'array', 'time', 'mem_per_cpu',
    'mem', 'gpus', 'wrap'
]

def not_avail(option):
    """Inform user that an option is not yet implemented"""
    option_name = option.split('=')[0] if '=' in option else option
    # Remove leading dashes and convert dashes to underscores
    clean_name = option_name.lstrip('-').replace('-', '_')
    if clean_name in active_sbatch_options:
        return  # Don't warn about implemented options
    print(f"Warning: Option '{option}' is not yet implemented")

def parse_sbatch_directives(filename: str) -> Dict[str, Any]:
    """
    Parse #SBATCH directives from a submission script and return resource requirements
    
    Returns:
        Dict with keys:
        - cpus: Number of CPUs required
        - memory_mb: Memory in MB
        - gpus: Number of GPUs required
        - time_minutes: Time limit in minutes
        - local_disk_gb: Local disk space in GB
        - job_name: Name of the job
    """
    resources = {
        'cpus': 1,
        'memory_mb': 1024,  # 1GB default
        'gpus': 0,
        'time_minutes': 60,
        'local_disk_gb': 0,
        'job_name': None
    }
    
    try:
        with open(filename, 'r') as f:
            content = f.read()
            
        # Parse CPU requirements
        if cpu_match := re.search(r'#SBATCH\s+--cpus-per-task=(\d+)', content):
            resources['cpus'] = int(cpu_match.group(1))
            
        # Parse memory requirements
        if mem_match := re.search(r'#SBATCH\s+--mem=(\d+)(M|G)?', content):
            amount = int(mem_match.group(1))
            unit = mem_match.group(2) or 'M'
            resources['memory_mb'] = amount * 1024 if unit == 'G' else amount
            
        # Parse GPU requirements
        if gpu_match := re.search(r'#SBATCH\s+--gpus=(\d+)', content):
            resources['gpus'] = int(gpu_match.group(1))
            
        # Parse time limit
        if time_match := re.search(r'#SBATCH\s+--time=(?:(\d+)-)?(\d+):(\d+):(\d+)', content):
            days = int(time_match.group(1) or 0)
            hours = int(time_match.group(2))
            minutes = int(time_match.group(3))
            resources['time_minutes'] = days * 1440 + hours * 60 + minutes
            
        # Parse local disk requirements
        if disk_match := re.search(r'#SBATCH\s+--tmp=(\d+)(M|G)?', content):
            amount = int(disk_match.group(1))
            unit = disk_match.group(2) or 'M'
            resources['local_disk_gb'] = amount // 1024 if unit == 'M' else amount

        # Parse job name
        if name_match := re.search(r'#SBATCH\s+--job-name[= ]"?([^"\n]+)"?', content):
            resources['job_name'] = name_match.group(1).strip()
            
    except FileNotFoundError:
        print(f"Error: Script file '{filename}' not found")
        sys.exit(1)
        
    return resources

def check_filesystem_references(text: str) -> List[Tuple[str, bool]]:
    """
    Search for words starting with '/' and check if they are filesystem folders.
    
    Args:
        text (str): The input text to search for potential filesystem references
        
    Returns:
        List[Tuple[str, bool]]: List of tuples containing (path, exists_flag)
    """
    pattern = r'\b/\w+(?:/\w+)*\b'
    potential_paths = re.findall(pattern, text)
    results = []
    
    for path in potential_paths:
        path_obj = Path(path)
        exists = path_obj.exists() and path_obj.is_dir()
        results.append((path, exists))
        
        if exists:
            warnings.warn(
                f"Found reference to existing filesystem path: {path}",
                category=UserWarning
            )
            
    return results

def analyze_text(text: str, ignore_paths: List[str] = None) -> dict:
    """
    Analyze text for filesystem references with additional options and detailed reporting.
    
    Args:
        text (str): The input text to analyze
        ignore_paths (List[str], optional): List of paths to ignore
        
    Returns:
        dict: Analysis results including found paths and statistics
    """
    ignore_paths = set(ignore_paths or [])
    results = check_filesystem_references(text)
    
    report = {
        "total_references": len(results),
        "existing_paths": [],
        "nonexistent_paths": [],
        "ignored_paths": [],
        "statistics": {
            "total": len(results),
            "existing": 0,
            "nonexistent": 0,
            "ignored": 0
        }
    }
    
    for path, exists in results:
        if path in ignore_paths:
            report["ignored_paths"].append(path)
            report["statistics"]["ignored"] += 1
        elif exists:
            report["existing_paths"].append(path)
            report["statistics"]["existing"] += 1
        else:
            report["nonexistent_paths"].append(path)
            report["statistics"]["nonexistent"] += 1
    
    return report

def validate_paths(script_path: str, shared_mount: str) -> bool:
    """
    Validate that all file system references in the script are within FOMO_MOUNT_SHARED
    and that FOMO_MOUNT_SHARED is mounted as a juicefs filesystem
    
    Returns:
        bool: True if all paths are valid, False otherwise
    """
    try:
        # Check if shared_mount exists and is mounted
        if not os.path.ismount(shared_mount):
            print(f"Error: {shared_mount} is not mounted")
            print("Please ensure the JuiceFS filesystem is properly mounted at this location")
            return False

        # Check if it's a JuiceFS mount
        try:
            with open('/proc/mounts', 'r') as f:
                mounts = f.read()
                mount_type = None
                for line in mounts.splitlines():
                    if shared_mount in line:
                        mount_type = line.split()[2]
                        break
                
                if mount_type != 'fuse.juicefs':
                    print(f"Error: {shared_mount} is not mounted as a JuiceFS filesystem")
                    print("The shared mount must be a JuiceFS filesystem")
                    print("Current mount type:", mount_type)
                    return False
        except Exception as e:
            print(f"Error checking mount type: {e}")
            return False

        with open(script_path, 'r') as f:
            script_content = f.read()
            
        # Get the real path of the shared mount
        shared_mount_path = os.path.realpath(shared_mount)
        
        # Analyze the script
        analysis = analyze_text(script_content)
        
        # Check each existing path
        for path in analysis['existing_paths']:
            real_path = os.path.realpath(path)
            if not real_path.startswith(shared_mount_path):
                print(f"Error: Path '{path}' is outside of shared mount {shared_mount}")
                return False
                
        return True
        
    except Exception as e:
        print(f"Error validating paths: {e}")
        return False

def parse_arguments():
    """Parse command line arguments similar to sbatch"""
    parser = argparse.ArgumentParser(description='Slurm-compatible batch job submission for AWS EC2')
    
    # Script argument
    parser.add_argument('script', help='Batch script file', nargs='?')
    parser.add_argument('--wrap', help='send a command instead of a script file')
    
    # Parallel run options
    parser.add_argument('-a', '--array', help='job array index values')
    parser.add_argument('-A', '--account', help='charge job to specified account')
    parser.add_argument('--bb', help='burst buffer specifications')
    parser.add_argument('--bbf', help='burst buffer specification file')
    parser.add_argument('-b', '--begin', help='defer job until HH:MM MM/DD/YY')
    parser.add_argument('--comment', help='arbitrary comment')
    parser.add_argument('--cpu-freq', help='requested cpu frequency (and governor)')
    parser.add_argument('-c', '--cpus-per-task', type=int, help='number of cpus required per task')
    parser.add_argument('-d', '--dependency', help='defer job until condition on jobid is satisfied')
    parser.add_argument('--deadline', help='remove job if no ending possible before this deadline')
    parser.add_argument('--delay-boot', type=int, help='delay boot for desired node features')
    parser.add_argument('-D', '--chdir', help='set working directory for batch script')
    parser.add_argument('-e', '--error', help='file for batch script\'s standard error')
    parser.add_argument('--export', help='specify environment variables to export')
    parser.add_argument('--get-user-env', action='store_true', help='load environment from local cluster')
    parser.add_argument('--gid', help='group ID to run job as (user root only)')
    parser.add_argument('--gres', help='required generic resources')
    parser.add_argument('--gres-flags', help='flags related to GRES management')
    parser.add_argument('-H', '--hold', action='store_true', help='submit job in held state')
    parser.add_argument('-i', '--input', help='file for batch script\'s standard input')
    parser.add_argument('-J', '--job-name', help='name of job')
    parser.add_argument('-k', '--no-kill', action='store_true', help='do not kill job on node failure')
    parser.add_argument('-L', '--licenses', help='required license, comma separated')
    parser.add_argument('-M', '--clusters', help='comma separated list of clusters')
    parser.add_argument('-m', '--distribution', help='distribution method for processes to nodes')
    parser.add_argument('--mail-type', help='notify on state change: BEGIN, END, FAIL or ALL')
    parser.add_argument('--mail-user', help='who to send email notification for job state changes')
    parser.add_argument('--mcs-label', help='mcs label if mcs plugin mcs/group is used')
    parser.add_argument('-n', '--ntasks', type=int, help='number of tasks to run')
    parser.add_argument('--nice', help='decrease scheduling priority by value')
    parser.add_argument('--no-requeue', action='store_true', help='do not permit the job to be requeued')
    parser.add_argument('--ntasks-per-node', type=int, help='number of tasks to invoke on each node')
    parser.add_argument('-N', '--nodes', help='number of nodes on which to run (N = min[-max])')
    parser.add_argument('-o', '--output', help='file for batch script\'s standard output')
    parser.add_argument('-O', '--overcommit', action='store_true', help='overcommit resources')
    parser.add_argument('-p', '--partition', help='partition requested')
    parser.add_argument('--parsable', action='store_true', help='outputs only the jobid and cluster name')
    parser.add_argument('--power', help='power management options')
    parser.add_argument('--priority', help='set the priority of the job to value')
    parser.add_argument('--profile', help='enable acct_gather_profile for detailed data')
    parser.add_argument('--propagate', help='propagate all [or specific list of] rlimits')
    parser.add_argument('-q', '--qos', help='quality of service')
    parser.add_argument('-Q', '--quiet', action='store_true', help='quiet mode (suppress informational messages)')
    parser.add_argument('--reboot', action='store_true', help='reboot compute nodes before starting job')
    parser.add_argument('--requeue', action='store_true', help='if set, permit the job to be requeued')
    parser.add_argument('-s', '--oversubscribe', action='store_true', help='over subscribe resources with other jobs')
    parser.add_argument('-S', '--core-spec', help='count of reserved cores')
    parser.add_argument('--signal', help='send signal when time limit within time seconds')
    parser.add_argument('--spread-job', action='store_true', help='spread job across as many nodes as possible')
    parser.add_argument('--switches', help='Optimum switches and max time to wait for optimum')
    parser.add_argument('--thread-spec', help='count of reserved threads')
    parser.add_argument('-t', '--time', help='time limit')
    parser.add_argument('--time-min', help='minimum time limit (if distinct)')
    parser.add_argument('--tres-bind', help='task to tres binding options')
    parser.add_argument('--tres-per-task', help='list of tres required per task')
    parser.add_argument('--uid', help='user ID to run job as (user root only)')
    parser.add_argument('--use-min-nodes', action='store_true', help='prefer the smaller count in a range')
    parser.add_argument('-v', '--verbose', action='count', default=0, help='verbose mode')
    parser.add_argument('-W', '--wait', action='store_true', help='wait for completion of submitted job')
    parser.add_argument('--wckey', help='wckey to run job under')

    # Constraint options
    parser.add_argument('--cluster-constraint', help='specify a list of cluster constraints')
    parser.add_argument('--contiguous', action='store_true', help='demand a contiguous range of nodes')
    parser.add_argument('-C', '--constraint', help='specify a list of constraints')
    parser.add_argument('-F', '--nodefile', help='request a specific list of hosts')
    parser.add_argument('--mem', help='minimum amount of real memory')
    parser.add_argument('--mincpus', type=int, help='minimum number of logical processors per node')
    parser.add_argument('--reservation', help='allocate resources from named reservation')
    parser.add_argument('--tmp', help='minimum amount of temporary disk')
    parser.add_argument('-w', '--nodelist', help='request a specific list of hosts')
    parser.add_argument('-x', '--exclude', help='exclude a specific list of hosts')

    # GPU scheduling options
    parser.add_argument('--cpus-per-gpu', type=int, help='number of CPUs required per allocated GPU')
    parser.add_argument('-G', '--gpus', help='count of GPUs required for the job')
    parser.add_argument('--gpu-bind', help='task to gpu binding options')
    parser.add_argument('--gpu-freq', help='frequency and voltage of GPUs')
    parser.add_argument('--gpus-per-node', help='number of GPUs required per allocated node')
    parser.add_argument('--gpus-per-socket', help='number of GPUs required per allocated socket')
    parser.add_argument('--gpus-per-task', help='number of GPUs required per spawned task')
    parser.add_argument('--mem-per-gpu', help='real memory required per allocated GPU')
    
    args = parser.parse_args()
    
    # Handle any provided options
    for arg in vars(args):
        value = getattr(args, arg)
        if value is not None and arg != 'script' and arg != 'verbose':  # Skip verbose option
            if isinstance(value, bool):
                if value:
                    not_avail(f"--{arg.replace('_', '-')}")
            elif isinstance(value, int):
                not_avail(f"--{arg.replace('_', '-')}={value}")
            elif isinstance(value, str):
                not_avail(f"--{arg.replace('_', '-')}={value}")
    
    return args

class EnvConfig:
    def __init__(self, filepath):
        """
        Initialize the EnvConfig object with a filepath and load the variables.
        
        Args:
            filepath (str): Path to the environment file
        """
        self.filepath = filepath
        self._config = {}
        self.load_config()

    def load_config(self):
        """Load or reload the configuration from the file."""
        try:
            with open(self.filepath, 'r') as file:
                for line in file:
                    # Skip empty lines and comments
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    
                    # Split on first '=' to handle values that might contain '='
                    if '=' in line:
                        key, value = line.split('=', 1)
                        # Clean up the variables
                        key = key.strip()
                        value = value.strip()
                        
                        # Store in config dictionary
                        self._config[key] = value
                        
                        # Dynamically set attribute
                        setattr(self, key, value)
                        
        except FileNotFoundError:
            print(f"Error: File {self.filepath} not found")
        except Exception as e:
            print(f"Error reading file: {e}")

    def reload(self):
        """Reload the configuration from the file."""
        self._config.clear()
        self.load_config()

    def get(self, key, default=None):
        """
        Get a configuration value by key.
        
        Args:
            key (str): The configuration key to look up
            default: The value to return if key is not found
        
        Returns:
            The value associated with the key, or default if not found
        """
        return self._config.get(key, default)

    def all(self):
        """
        Get all configuration values as a dictionary.
        
        Returns:
            dict: All configuration values
        """
        return self._config.copy()

    def __str__(self):
        """String representation of the configuration."""
        return '\n'.join(f'{k}={v}' for k, v in self._config.items())

def test_vars():
    """Test the EnvConfig class with some variables."""
    # Create a config object
    env_config = EnvConfig('default.env')  # or '.env' if renamed
    
    # Print all variables
    print("Current variables:")
    print(env_config)
    
    # Access individual variables (multiple ways):
    #print("\nAccessing individual variables:")
    # Method 1: Direct attribute access
    # if hasattr(env_config, 'FOMO_EC2TYPE'):
    #    print(f"EC2 Type: {env_config.FOMO_EC2TYPE}")
    
    # Method 2: Dictionary-style access with get()
    #print(f"Redis Password: {env_config.get('FOMO_REDIS_PW', 'Not set')}")
            
    # Get all variables as a dictionary
    # all_vars = env_config.all()
    #print("\nAll variables as dictionary:")
    #for key, value in all_vars.items():
    #    print(f"{key}: {value}")


# Main execution
def generate_cloud_init(script_path: str, username: str, env_config: EnvConfig) -> str:
    """Generate cloud-init script for the EC2 instance
    
    Args:
        script_path (str): Path to the submission script
        username (str): Username to set up on the instance
        env_config (EnvConfig): Environment configuration object
        
    Returns:
        str: Path to generated cloud-init script
    """
    if not username or not isinstance(username, str):
        raise ValueError(f"Invalid username: {username}")
    """Generate cloud-init script for the EC2 instance"""
    work_dir = os.path.join(os.path.dirname(__file__), '..', '.work')
    os.makedirs(work_dir, exist_ok=True)
    
    # Base cloud-init template
    with open(os.path.join(work_dir, 'ec2-cloud-init-node.txt'), 'r') as f:
        cloud_init = f.read()
        
    # Generate user/group setup
    def get_user_groups(username):
        #print ("...entering get_user_groups")
        """Get all groups for a user using only Python standard library"""
        try:
            # Get user's password database entry for primary group
            pw_entry = pwd.getpwnam(username)
            primary_gid = pw_entry.pw_gid
            primary_group = grp.getgrgid(primary_gid).gr_name

            # Get all groups the user is in using os.getgrouplist
            gids = os.getgrouplist(username, primary_gid)
            
            # Convert GIDs to group names, excluding primary group
            secondary_groups = []
            for gid in gids:
                try:
                    group_name = grp.getgrgid(gid).gr_name
                    if group_name != primary_group:
                        secondary_groups.append(group_name)
                except KeyError:
                    continue
            
            return primary_group, sorted(secondary_groups)
        except (KeyError, AttributeError):
            return None, []

    def analyze_ownership(directory, username):
        #print ("...entering analyze_ownership")

        users = defaultdict(set)    # {username: {uid}}
        groups = defaultdict(set)   # {groupname: {gid}}
        user_groups = {}           # {username: (primary_group, [secondary_groups])}
        
        # Get current user's info first
        try:
            current_user_info = pwd.getpwnam(username)
            current_uid = current_user_info.pw_uid
            current_gid = current_user_info.pw_gid
            
            # Get user's groups
            primary_group, secondary_groups = get_user_groups(username)
            if not primary_group:
                print(f"Error: Could not get group information for user {username}")
                sys.exit(1)
                
            # Always add current user and their primary group
            users[username].add(current_uid)
            user_groups[username] = (primary_group, secondary_groups)
            
            # Add primary group
            try:
                primary_group_info = grp.getgrgid(current_gid)
                groups[primary_group_info.gr_name].add(current_gid)
            except KeyError:
                print(f"Warning: Could not find group info for GID {current_gid}")

            # Add any groups that own directories in the shared mount AND 
            # that the current user is a member of
            try:
                # Get all groups the current user is a member of
                user_group_names = set(secondary_groups)
                user_group_names.add(primary_group)

                shared_dirs = [d for d in Path(directory).iterdir() if d.is_dir()]
                for dir_path in shared_dirs:
                    dir_stat = dir_path.stat()
                    dir_gid = dir_stat.st_gid
                    try:
                        group_info = grp.getgrgid(dir_gid)
                        group_name = group_info.gr_name
                        # Only add groups that:
                        # 1. Have actual names (not just numeric)
                        # 2. The current user is a member of
                        # 3. Own at least one non-hidden directory
                        if (not group_name.isdigit() and 
                            group_name in user_group_names):
                            groups[group_name].add(dir_gid)
                    except KeyError:
                        continue
            except (OSError, PermissionError) as e:
                print(f"Warning: Could not scan shared directory: {e}")
                
        except KeyError:
            print(f"Error: Could not get user information for {username}")
            sys.exit(1)
            
        # Get entries in directory (excluding hidden files)
        dir_path = Path(directory)
        try:
            entries = [e for e in dir_path.iterdir() if not e.name.startswith('.')]
            if not entries:
                print(f"Error: No non-hidden files found in shared mount directory '{directory}'")
                print("The shared mount directory appears to be empty.")
                print("Please ensure:")
                print("1. The JuiceFS filesystem is properly mounted")
                print("2. Files have been copied to the shared directory")
                print("3. You have proper permissions to access the files")
                sys.exit(1)
        except FileNotFoundError:
            print(f"Error: Shared mount directory '{directory}' does not exist.")
            print("Please ensure the directory exists and is accessible.")
            print("You may need to:")
            print("1. Check if the shared filesystem is mounted")
            print("2. Verify the FOMO_MOUNT_SHARED path in your .env file")
            print("3. Ensure you have proper permissions to access the directory")
            sys.exit(1)

        for entry in entries:
            stat = entry.stat()
            uid, gid = stat.st_uid, stat.st_gid
            
            try:
                # Get user info
                user = pwd.getpwuid(uid).pw_name
                users[user].add(uid)
                
                # Get group info - only add if it has a name
                try:
                    group_info = grp.getgrgid(gid)
                    group_name = group_info.gr_name
                    # Only add groups with actual names (not just numeric)
                    if not group_name.isdigit():
                        groups[group_name].add(gid)
                    
                    # Store user's group info
                    primary_group, secondary_groups = get_user_groups(user)
                    if primary_group:
                        user_groups[user] = (primary_group, secondary_groups)
                        
                        # Add all named groups this user belongs to
                        if not primary_group.isdigit():
                            groups[primary_group].add(grp.getgrnam(primary_group).gr_gid)
                        for group in secondary_groups:
                            if not group.isdigit():
                                try:
                                    groups[group].add(grp.getgrnam(group).gr_gid)
                                except KeyError:
                                    pass
                except KeyError:
                    # Skip groups that can't be looked up
                    pass
            except KeyError:
                # Skip users that can't be looked up
                pass
        
        return users, groups, user_groups

    def generate_script(users, groups, user_groups, username):
        #print ("...entering generate_script")
        # print(users, groups, user_groups, username)
        """Generate user and group setup commands to be added to cloud-init"""
        script_content = []
        
        # Write group creation commands
        script_content.append('\n# Create groups')
        
        # Get all groups the user is a member of
        primary_group, secondary_groups = user_groups.get(username, (None, []))
        
        all_user_groups = set(secondary_groups)
        if primary_group is not None:
            all_user_groups.add(primary_group)
            
        # Create all user's groups that have GID >= 200
        for groupname in sorted(all_user_groups):
            try:
                group_info = grp.getgrnam(groupname)
                gid = group_info.gr_gid
                # Skip system groups
                if gid < 200:
                    continue
                script_content.append(f'groupadd -g {gid} {groupname} 2>/dev/null || handle_error "Group {groupname} already exists"')
            except KeyError:
                continue
        
        script_content.append('\n# Create users with their groups')
        # Write user creation commands with all their groups
        for username, uids in sorted(users.items()):
            for uid in sorted(uids):
                # Skip system UIDs
                if uid < 1000:
                    continue
                    
                primary_group, secondary_groups = user_groups.get(username, (None, []))
                if primary_group:

                    # Build useradd command
                    cmd = f'useradd -u {uid} -g {primary_group}'
                    
                    # Filter out secondary groups with GID < 200
                    valid_groups = set()
                    for group in secondary_groups:
                        try:
                            group_info = grp.getgrnam(group)
                            if group_info.gr_gid >= 200:
                                valid_groups.add(group)
                        except KeyError:
                            continue
                            
                    # Add filtered secondary groups if they exist
                    if valid_groups:
                        cmd += f' -G {",".join(valid_groups)}'
                    
                    cmd += f' -d /home/{username} -m {username} 2>/dev/null || handle_error "User {username} already exists"'
                    script_content.append(cmd)
                    
                    # Set permissions on home directory
                    #script_content.append(f'chown {username}:{primary_group} /home/{username}')
                    #script_content.append(f'chmod 750 /home/{username}')
                    
                    # Add SSH key setup
                    script_content.append(f'\n# Setup SSH keys for {username}')
                    script_content.append(f'find_and_copy_ssh_keys {username}')

        return '\n'.join(script_content)
    
    shared_mount = env_config.get('FOMO_MOUNT_SHARED')
    users, groups, user_groups = analyze_ownership(shared_mount, username)
    
    # Add helper functions at the start
    cloud_init += '''
# Helper functions
handle_error() {
  local exit_code=$?
  echo "Warning: $1 (exit code: $exit_code)"
  return 0  # Continue script execution
}

find_and_copy_ssh_keys() {
  local target_user=$1
  local found_keys=false
  
  # List of users to check for SSH keys
  local users_to_check=("ec2-user" "rocky" "ubuntu")
  
  for source_user in "${users_to_check[@]}"; do
    if [[ -f "/home/$source_user/.ssh/authorized_keys" ]]; then
      echo "Found SSH keys from $source_user"
      
      # Create .ssh directory with proper permissions
      mkdir -p "/home/$target_user/.ssh"
      chmod 700 "/home/$target_user/.ssh"
      
      # Copy and set proper permissions on authorized_keys
      cp "/home/$source_user/.ssh/authorized_keys" "/home/$target_user/.ssh/"
      chmod 600 "/home/$target_user/.ssh/authorized_keys"
      chown -R "$target_user:$(id -gn $target_user)" "/home/$target_user/.ssh"
      
      found_keys=true
      break
    fi
  done
  
  if [[ "$found_keys" = false ]]; then
    echo "No SSH keys found from any of the standard users"
  fi
}
'''

    # Generate user setup script content
    setup_script = generate_script(users, groups, user_groups, username)
    
    # Add submission script
    with open(script_path, 'r') as f:
        submission_script = f.read()
        
    # Combine all parts
    cloud_init += "\n# User and group setup\n"
    cloud_init += setup_script + "\n"  # Add newline after setup script

    # Add environment setup
    module_path = env_config.get('FOMO_MODULE_PATH')
    cloud_init += f'sudo -u {username} echo "export MODULEPATH={module_path}" > /home/{username}/.bashrc \n'
    
    # Add directory creation for shared mount
    cloud_init += f"\n# Create shared mount directory\n"
    cloud_init += f"mkdir -p {env_config.get('FOMO_MOUNT_SHARED')}\n"
    cloud_init += f"chown {username}:{username} {env_config.get('FOMO_MOUNT_SHARED')}\n"
    
    # Add submission script with proper ownership
    now=int(time.time())
    jobbase = os.path.join(env_config.get('FOMO_MOUNT_SHARED'), f'job-{username}-{now}')
    cloud_init += "\n# Submission script\n"
    cloud_init += f"cat > {jobbase}.sh << 'EOL'\n{submission_script}\nEOL\n"
    cloud_init += f"chown {username}:{username} {jobbase}.sh\n"
    cloud_init += f"\nchmod +x {jobbase}.sh\n"
    cloud_init += f'sudo -u {username} bash --login -c "{jobbase}.sh > {jobbase}.out 2>&1"\n'
    cloud_init += f"chown {username}:{username} {jobbase}.out\n"
    
    # Add cleanup logic
    cloud_init += """
# Install AWS CLI if not present
if ! command -v aws >/dev/null 2>&1; then
  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
  unzip -q awscliv2.zip
  ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli
  rm -rf aws awscliv2.zip
fi

# Get instance ID from metadata service
ETOKEN=$(curl -sX PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600")
INSTANCE_ID=$(curl -sH "X-aws-ec2-metadata-token: $ETOKEN" http://169.254.169.254/latest/meta-data/instance-id)
REGION=$(curl -sH "X-aws-ec2-metadata-token: $ETOKEN" http://169.254.169.254/latest/meta-data/placement/region)

# Calculate minutes until next hour boundary based on start time
CURRENT_TIME=$(date +%s)
ELAPSED_SECONDS=$((CURRENT_TIME - START_TIME))
ELAPSED_MINUTES=$((ELAPSED_SECONDS / 60))
MINUTES_IN_HOUR=60
MINUTES_LEFT=$((MINUTES_IN_HOUR - (ELAPSED_MINUTES % MINUTES_IN_HOUR)))

if [ $MINUTES_LEFT -gt 5 ]; then
    sleep $(($MINUTES_LEFT - 5))m
fi

# Cleanup and terminate instance
fusermount3 -u ${FOMO_MOUNT_SHARED}
aws ec2 terminate-instances --region ${REGION} --instance-ids ${INSTANCE_ID}
shutdown -h now
"""
    
    # Write to file
    output_path = os.path.join(work_dir, f'ec2-cloud-init-node-{username}.txt')
    with open(output_path, 'w') as f:
        f.write(cloud_init)
        
    return output_path

import boto3
import botocore.exceptions
from botocore.exceptions import ClientError

def find_instance_type(resources: Dict[str, Any]) -> str:
    """Find the smallest suitable EC2 instance type based on resource requirements"""
    try:
        ec2_client = boto3.client('ec2')
        
        # Get allowed instance families from config
        allowed_families = env_config.get('FOMO_EC2_FAMILIES_NODE', '').split(',')
        if not allowed_families or not allowed_families[0]:
            print("Warning: FOMO_EC2_FAMILIES_NODE not set, using all instance families")
            allowed_families = ['*']

        suitable_types = []
        paginator = ec2_client.get_paginator('describe_instance_types')
        
        # Build instance family patterns
        filters = []
        if '*' not in allowed_families:
            instance_patterns = [f"{family}.*" for family in allowed_families]
            filters.append({
                'Name': 'instance-type',
                'Values': instance_patterns
            })

        for page in paginator.paginate(Filters=filters):
            for itype in page['InstanceTypes']:
                # Basic requirements check
                vcpus = itype['VCpuInfo']['DefaultVCpus']
                memory = itype['MemoryInfo']['SizeInMiB']
                
                if vcpus >= resources['cpus'] and memory >= resources['memory_mb']:
                    # GPU requirements check if needed
                    if resources['gpus'] > 0:
                        gpu_info = itype.get('GpuInfo', {}).get('Gpus', [])
                        if not gpu_info:
                            continue
                        gpu_count = sum(gpu.get('Count', 0) for gpu in gpu_info)
                        if gpu_count < resources['gpus']:
                            continue
                        if resources.get('gpu_vram'):
                            gpu_memory = gpu_info[0].get('MemoryInfo', {}).get('SizeInMiB', 0)
                            if gpu_memory < resources['gpu_vram'] * 1024:
                                continue
                    
                    # Local disk check if needed
                    if resources.get('local_disk_gb', 0) > 0:
                        storage_info = itype.get('InstanceStorageInfo', {})
                        if not storage_info or storage_info.get('TotalSizeInGB', 0) < resources['local_disk_gb']:
                            continue
                    
                    suitable_types.append(itype)

        if not suitable_types:
            # Get allowed instance families from config
            allowed_families = env_config.get('FOMO_EC2_FAMILIES_NODE', '').split(',')
            if not allowed_families or not allowed_families[0]:
                print("Warning: FOMO_EC2_FAMILIES_NODE not set, defaulting to t3.small for minimal requirements")
                if resources['cpus'] <= 1 and resources['memory_mb'] <= 2048:
                    return "t3.small"
                return "c7i.large"

            # For minimal requirements, find smallest allowed instance
            if resources['cpus'] <= 1 and resources['memory_mb'] <= 2048:
                # Try each family in order
                for family in allowed_families:
                    try:
                        response = ec2_client.describe_instance_types(
                            Filters=[
                                {'Name': 'instance-type', 'Values': [f"{family}.*"]},
                                {'Name': 'vcpu-info.default-vcpus', 'Values': ['1', '2']},
                                {'Name': 'memory-info.size-in-mib', 'Values': ['1024', '2048', '4096']}
                            ]
                        )
                        if response['InstanceTypes']:
                            # Sort by vCPUs then memory to get smallest
                            instances = sorted(response['InstanceTypes'], 
                                            key=lambda x: (x['VCpuInfo']['DefaultVCpus'], 
                                                         x['MemoryInfo']['SizeInMiB']))
                            return instances[0]['InstanceType']
                    except ClientError:
                        continue

            print(f"No suitable small instance found in families: {','.join(allowed_families)}")
            print("Using c7i.large as fallback")
            return "c7i.large"

        # Sort by vCPUs, memory, and GPU memory (if applicable) to get smallest viable instance
        if resources['gpus'] > 0:
            suitable_types.sort(
                key=lambda x: (
                    x['VCpuInfo']['DefaultVCpus'],
                    x['MemoryInfo']['SizeInMiB'],
                    x.get('GpuInfo', {}).get('Gpus', [{}])[0].get('MemoryInfo', {}).get('SizeInMiB', 0)
                )
            )
        else:
            suitable_types.sort(
                key=lambda x: (
                    x['VCpuInfo']['DefaultVCpus'],
                    x['MemoryInfo']['SizeInMiB']
                )
            )

        return suitable_types[0]['InstanceType']

    except (ClientError, botocore.exceptions.TokenRetrievalError) as e:
        if "Token has expired" in str(e):
            print("Error: AWS CLI is not authenticated. Executing 'aws sso login --no-browser'")
            try:
                subprocess.run(["aws", "sso", "login", "--no-browser"], check=True)
                # Retry the operation after login
                return find_instance_type(resources)
            except subprocess.CalledProcessError as sso_error:
                print(f"Error during SSO login: {sso_error}")
                sys.exit(1)
        else:
            print(f"Error finding instance type: {e}")
            return "t3.small"

def find_ami_image(env_config: EnvConfig, arch: str = 'x86_64') -> str:
    """Find latest AMI ID matching FOMO_AMINAME_NODE"""
    try:
        ec2_client = boto3.client('ec2')
        filters = [
            {'Name': 'name', 'Values': ['Rocky-9-EC2-Base*', 'Rocky-10-EC2-Base*']},
            {'Name': 'architecture', 'Values': [arch]},
            {'Name': 'virtualization-type', 'Values': ['hvm']},
            {'Name': 'state', 'Values': ['available']}
        ]
        
        response = ec2_client.describe_images(
            Owners=['792107900819'],  # Rocky Linux owner ID
            Filters=filters
        )
        
        if not response['Images']:
            print(f"Error: No matching AMI found for architecture {arch}")
            sys.exit(1)
            
        # Sort by deprecation time to get the latest
        images = sorted(response['Images'], 
                      key=lambda x: x.get('DeprecationTime', '9999-12-31'),  # Default to far future if no deprecation
                      reverse=True)
        
        ami_id = images[0]['ImageId']
        ami_name = images[0]['Name'].split('/')[-1]
        print(f'Using AMI name {ami_name}')
        return ami_id
        
    except ClientError as e:
        print(f"Error finding AMI: {e}")
        sys.exit(1)

def launch_ec2_instance(cloud_init_path: str, resources: Dict[str, Any], env_config: EnvConfig, username: str) -> str:
    """Launch EC2 instance with the specified resources"""
    try:
        # Find appropriate instance type
        instance_type = find_instance_type(resources)
        #print(f"Selected instance type: {instance_type}")
        
        # Get instance type architecture first
        ec2_client = boto3.client('ec2')
        instance_response = ec2_client.describe_instance_types(
            InstanceTypes=[instance_type]
        )
        if not instance_response['InstanceTypes']:
            print("Error: Could not get instance type details")
            sys.exit(1)
            
        instance_arch = instance_response['InstanceTypes'][0]['ProcessorInfo']['SupportedArchitectures'][0]
        
        # Find matching AMI for the instance architecture
        ami_id = find_ami_image(env_config, instance_arch)
        
        # Get AMI details for output
        ami_response = ec2_client.describe_images(ImageIds=[ami_id])
        ami_arch = instance_arch
        print(f"Selected AMI: {ami_id} ({ami_arch})")
        
        # Get instance type architecture
        instance_response = ec2_client.describe_instance_types(
            InstanceTypes=[instance_type]
        )
        if not instance_response['InstanceTypes']:
            print("Error: Could not get instance type details")
            sys.exit(1)
            
        instance_arch = instance_response['InstanceTypes'][0]['ProcessorInfo']['SupportedArchitectures'][0]
        
        # If architectures don't match, we need to find a new AMI with matching architecture
        if instance_arch != ami_arch:
            print(f"Architecture mismatch: Instance type {instance_type} is {instance_arch} but AMI is {ami_arch}")
            print(f"Finding new AMI with {instance_arch} architecture...")
            
            # Find new AMI with matching architecture
            filters = [{
                'Name': 'name',
                'Values': [env_config.get('FOMO_AMINAME_NODE')]
            }, {
                'Name': 'architecture',
                'Values': [instance_arch]
            }]
            
            ami_response = ec2_client.describe_images(
                Owners=['792107900819'],  # Rocky Linux owner ID
                Filters=filters
            )
            
            if not ami_response['Images']:
                print(f"Error: No AMI found matching {instance_arch} architecture")
                sys.exit(1)
                
            # Sort by creation date to get the latest
            images = sorted(ami_response['Images'], 
                          key=lambda x: x['CreationDate'],
                          reverse=True)
            
            ami_id = images[0]['ImageId']
            ami_arch = instance_arch
            print(f"Selected new AMI: {ami_id} ({ami_arch})")
        
        # Read user data from file
        with open(cloud_init_path, 'r') as f:
            user_data = f.read()
        
        # Create EC2 client
        region = env_config.get('FOMO_AWS_REGION')
        if not region or '${' in region:
            region = 'us-west-2'  # Default to us-west-2 if not set
        ec2_client = boto3.client('ec2', region_name=region)
        
        # Get security group ID
        security_group = env_config.get('FOMO_EC2_SEC_GR')
        if not security_group:
            print("Error: FOMO_EC2_SEC_GR not set in environment config")
            sys.exit(1)
            
        # Verify security group exists
        try:
            sg_response = ec2_client.describe_security_groups(GroupNames=[security_group])
            security_group_id = sg_response['SecurityGroups'][0]['GroupId']
        except ClientError as e:
            print(f"Error: Security group '{security_group}' not found: {e}")
            sys.exit(1)

        # Get key name from config
        key_name = env_config.get('FOMO_EC2_KEY_NAME')
        if not key_name:
            print("Error: FOMO_EC2_KEY_NAME not set in environment config")
            sys.exit(1)

        # Determine job name
        base_name = resources.get('job_name')
        if not base_name:
            if args.wrap:
                base_name = 'sbatch-wrap'
            else:
                base_name = os.path.splitext(os.path.basename(args.script))[0]

        # Check for existing instances with same name and increment if needed
        existing_instances = ec2_client.describe_instances(
            Filters=[
                {'Name': 'tag:Name', 'Values': [f"{base_name}*"]},
                {'Name': 'tag:fomo-hpc', 'Values': ['true']},
                {'Name': 'instance-state-name', 'Values': ['pending', 'running', 'stopping', 'stopped']}
            ]
        )

        # Get all existing names that match the pattern base_name or base_name<number>
        existing_names = set()
        max_counter = 0
        for reservation in existing_instances['Reservations']:
            for instance in reservation['Instances']:
                for tag in instance['Tags']:
                    if tag['Key'] == 'Name' and tag['Value'].startswith(base_name):
                        existing_names.add(tag['Value'])
                        # Extract number suffix if present
                        if tag['Value'] != base_name:
                            try:
                                counter = int(tag['Value'][len(base_name):])
                                max_counter = max(max_counter, counter)
                            except ValueError:
                                continue

        # Use next number after highest existing number
        job_name = f"{base_name}{max_counter + 1}" if max_counter > 0 or base_name in existing_names else base_name

        # Launch instance
        response = ec2_client.run_instances(
            ImageId=ami_id,
            InstanceType=instance_type,
            MinCount=1,
            MaxCount=1,
            KeyName=key_name,
            SecurityGroupIds=[security_group_id],
            UserData=user_data,
            TagSpecifications=[{
                'ResourceType': 'instance',
                'Tags': [
                    {'Key': 'Name', 'Value': job_name},
                    {'Key': 'fomo-hpc', 'Value': 'true'},
                    {'Key': 'User', 'Value': username}
                ]
            }]
        )
        
        instance = response['Instances'][0]
        instance_id = instance['InstanceId']
        az = instance['Placement']['AvailabilityZone']
        print(f"Instance launched in availability zone: {az}")
        
        # Get detailed instance info for output
        instance_info = ec2_client.describe_instance_types(
            InstanceTypes=[instance_type]
        )['InstanceTypes'][0]
        
        vcpus = instance_info['VCpuInfo']['DefaultVCpus']
        memory = instance_info['MemoryInfo']['SizeInMiB'] / 1024  # Convert to GB
        ephemeral = instance_info.get('InstanceStorageInfo', {}).get('TotalSizeInGB', 0)
        gpu_info = instance_info.get('GpuInfo', {}).get('Gpus', [{}])[0]
        gpu_name = gpu_info.get('Name', 'N/A')
        gpu_memory = gpu_info.get('MemoryInfo', {}).get('SizeInMiB', 0) / 1024  # Convert to GB
        
        print(f"Selected instance type: {instance_type} (Architecture: {instance_arch}, vCPUs: {vcpus}, "
              f"Mem: {memory:.1f} GB, Ephemeral: {ephemeral} GB, GPU: {gpu_name} ({gpu_memory:.1f} GB))")
        
        #print(f"Searching for AMI with OS: {env_config.get('FOMO_AMINAME_NODE')}, Architecture: {instance_arch}")
        #print(f"AMI Image found: {ami_id} ({ami_response['Images'][0]['Name']})")
        
        print(f"Instance {instance_id} is starting. Waiting for public IP...")
        
        # Wait for instance to be ready and get its public IP
        waiter = ec2_client.get_waiter('instance_running')
        try:
            waiter.wait(InstanceIds=[instance_id])
        except KeyboardInterrupt:
            print("\nOperation interrupted by user. Cleaning up...")
            if instance_id:
                try:
                    ec2_client.terminate_instances(InstanceIds=[instance_id])
                    print(f"Terminated instance {instance_id}")
                except Exception as e:
                    print(f"Error terminating instance: {e}")
            sys.exit(1)
        
        instance = ec2_client.describe_instances(InstanceIds=[instance_id])['Reservations'][0]['Instances'][0]
        public_ip = instance.get('PublicIpAddress')
        public_dns = instance.get('PublicDnsName')
        
        if not public_ip:
            print("Error: Instance did not receive a public IP address")
            sys.exit(1)
            
        # Print SSH connection commands
        print("\nWait a minute and run one of these SSH commands to connect:")
        # Get AWS account and user info
        sts_client = boto3.client('sts')
        caller_identity = sts_client.get_caller_identity()
        aws_account = caller_identity['Account']
        aws_user = caller_identity['Arn'].split('/')[-1].split('@')[0]
        
        key_file = f"~/.ssh/auto-ec2-{aws_account}-{aws_user}.pem"
        #key_file = os.path.expanduser(key_file)
        print(f"ssh -i {key_file} {username}@{public_dns}")
        print(f"ssh -i {key_file} {username}@{public_ip}")

        return instance_id, instance_type, ami_id, ami_response['Images'][0]['Name'], public_ip, public_dns
        
    except ClientError as e:
        print(f"Error launching EC2 instance: {e}")
        sys.exit(1)

if __name__ == "__main__":
    instance_id = None
    username = None
    try:
        # Get current username first
        username = getpass.getuser()
        if not username:
            print("Error: Could not determine username")
            sys.exit(1)

        # Parse command line arguments
        args = parse_arguments()
        
        # Create a dictionary of CLI options that will override script options
        cli_options = {}
        for option in active_sbatch_options:
            value = getattr(args, option.replace('-', '_'), None)
            if value is not None:
                cli_options[option] = value

        # Handle wrap command
        if args.wrap:
            # Create a temporary script file
            import tempfile
            script_file = tempfile.NamedTemporaryFile(mode='w', prefix='sbatch-wrap-', suffix='.sh', delete=False)
            script_file.write('#!/bin/bash\n')
            script_file.write(args.wrap + '\n')
            script_file.close()
            args.script = script_file.name
        elif not args.script:
            print("Error: Either a script file or --wrap must be specified")
            sys.exit(1)
        
        # Load environment configuration
        env_file = '.env' if os.path.exists('.env') else 'default.env'
        env_config = EnvConfig(env_file)
        
        # Validate paths
        if not validate_paths(args.script, env_config.get('FOMO_MOUNT_SHARED')):
            sys.exit(1)
            
        # Initialize resources with defaults
        resources = {
            'cpus': 1,
            'memory_mb': 1024,  # 1GB default
            'gpus': 0,
            'time_minutes': 60,
            'local_disk_gb': 0,
            'job_name': None
        }

        # Parse SBATCH directives from the script only if no CLI options override them
        script_resources = parse_sbatch_directives(args.script)
        
        # Update resources with script values only for options not specified in CLI
        for key, value in script_resources.items():
            if key not in [opt.replace('-', '_') for opt in cli_options.keys()]:
                resources[key] = value

        # Apply CLI options (these take precedence)
        for option, value in cli_options.items():
            if option == 'mem':
                # Handle memory specification (convert to MB)
                if isinstance(value, str):
                    value = value.upper()  # Convert to uppercase for consistent handling
                    if value.endswith('G') or value.endswith('GB'):
                        # Remove G or GB and convert to MB
                        value = int(value.rstrip('GB')) * 1024
                    elif value.endswith('M') or value.endswith('MB'):
                        # Remove M or MB and keep as MB
                        value = int(value.rstrip('MB'))
                    else:
                        # Assume MB if no unit specified
                        value = int(value)
                resources['memory_mb'] = value
            elif option == 'cpus_per_task':
                resources['cpus'] = value
            elif option == 'time':
                # Convert time format to minutes
                if ':' in value:
                    parts = value.split(':')
                    if len(parts) == 2:
                        hours, minutes = map(int, parts)
                        resources['time_minutes'] = hours * 60 + minutes
                    elif len(parts) == 3:
                        hours, minutes, _ = map(int, parts)
                        resources['time_minutes'] = hours * 60 + minutes
                else:
                    resources['time_minutes'] = int(value)
            elif option == 'gpus':
                resources['gpus'] = int(value)
            elif option == 'job_name':
                resources['job_name'] = value
        
        # Generate cloud-init script
        cloud_init_path = generate_cloud_init(args.script, username, env_config)
        #sys.exit(1)
        
        # Launch EC2 instance
        instance_id, instance_type, ami_id, ami_name, public_ip, public_dns = launch_ec2_instance(cloud_init_path, resources, env_config, username)
        
    
    except KeyboardInterrupt:
        print("\nOperation interrupted by user. Cleaning up...")
        if instance_id:
            try:
                ec2_client = boto3.client('ec2')
                ec2_client.terminate_instances(InstanceIds=[instance_id])
                print(f"Terminated instance {instance_id}")
            except Exception as e:
                print(f"Error terminating instance: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\nError: {e}")
        if instance_id:
            try:
                ec2_client = boto3.client('ec2')
                ec2_client.terminate_instances(InstanceIds=[instance_id])
                print(f"Terminated instance {instance_id}")
            except Exception as term_error:
                print(f"Error terminating instance: {term_error}")
        sys.exit(1)
    finally:
        # Clean up temporary wrap script if it exists
        if args.wrap and 'script_file' in locals():
            try:
                os.unlink(script_file.name)
            except Exception:
                pass
